{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0d9129",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5155324-5f17-43e2-be97-d0e532aa1383",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb16266-cc6d-44b4-a335-7c6f59b66eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4a385e-56e6-428a-be00-05620296abbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_data(measure_path):\n",
    "  measure = torch.load(measure_path)\n",
    "  val_losses = measure[\"val_losses\"]\n",
    "  train_scores = measure[\"train_scores\"]\n",
    "  metric_names = [\"map\", \"map_50\", \"map_75\", \"map_medium\", \"map_large\", \"mar_1\", \"mar_10\", \"mar_100\", \"mar_medium\", \"mar_large\"]\n",
    "  maps, map_50s, map_75s, map_mediums, map_larges, mar_1s, mar_10s, mar_100s, mar_mediums, mar_larges = \\\n",
    "    zip(*[[score.item() for name, score in train_score.items() if name in metric_names] for train_score in train_scores])\n",
    "  return {\n",
    "    \"val_loss\": val_losses,\n",
    "    \"map\": maps,\n",
    "    \"map_50\": map_50s,\n",
    "    \"map_75\": map_75s,\n",
    "    \"map_medium\": map_mediums,\n",
    "    \"map_large\": map_larges,\n",
    "    \"mar_1\": mar_1s,\n",
    "    \"mar_10\": mar_10s,\n",
    "    \"mar_100\": mar_100s,\n",
    "    \"mar_medium\": mar_mediums,\n",
    "    \"mar_large\": mar_larges,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a5986-7461-41ac-b0f1-d1a39aa0c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure_file in os.listdir(\"measures\"):\n",
    "  filename = measure_file.split(\".\")[0]\n",
    "  measure_path = os.path.join(\"measures\", measure_file)\n",
    "  data = measure_data(measure_path)\n",
    "  df = pd.DataFrame(data)\n",
    "  df.to_csv(f\"{filename}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34e892-af35-4c50-b9e8-ef867d26b83d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ea52bf-6bc7-4e62-8603-d5b1a28f55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data(last_checkpoint_path):\n",
    "  last_checkpoint = torch.load(last_checkpoint_path, map_location=\"cuda\")\n",
    "    \n",
    "  train_losses = last_checkpoint[\"train_losses\"]\n",
    "  val_scores = last_checkpoint[\"scores\"]\n",
    "    \n",
    "  metric_names = [\"map\", \"map_50\", \"map_75\", \"map_medium\", \"map_large\", \"mar_1\", \"mar_10\", \"mar_100\", \"mar_medium\", \"mar_large\"]\n",
    "  maps, map_50s, map_75s, map_mediums, map_larges, mar_1s, mar_10s, mar_100s, mar_mediums, mar_larges = \\\n",
    "    zip(*[[score.item() for name, score in val_score.items() if name in metric_names] for val_score in val_scores])\n",
    "    \n",
    "  return {\n",
    "    \"train/loss\": train_losses,\n",
    "    \"val/map\": maps,\n",
    "    \"val/map_50\": map_50s,\n",
    "    \"val/map_75\": map_75s,\n",
    "    \"val/map_medium\": map_mediums,\n",
    "    \"val/map_large\": map_larges,\n",
    "    \"val/mar_1\": mar_1s,\n",
    "    \"val/mar_10\": mar_10s,\n",
    "    \"val/mar_100\": mar_100s,\n",
    "    \"val/mar_medium\": mar_mediums,\n",
    "    \"val/mar_large\": mar_larges,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9c1b39-eeee-48ba-8811-fe9185f1594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "models = [\"fasterrcnn\", \"retinanet\"]\n",
    "parts = [\"full\", \"head\", \"split-b1e-04-h1e-02\"]\n",
    "\n",
    "dirs = [f\"{model}-{part}\" for model in models for part in parts]\n",
    "\n",
    "for dir in dirs:\n",
    "  if dir == \"fasterrcnn-split-b1e-04-h1e-02\":\n",
    "    continue\n",
    "\n",
    "  checkpoints_path = os.path.join(dir, \"checkpoints.json\")\n",
    "  with open(checkpoints_path, \"r\") as f:\n",
    "    checkpoints = {int(epoch): path for epoch, path in json.load(f).items()}\n",
    "\n",
    "  last_checkpoint_path = checkpoints[max(checkpoints.keys())]\n",
    "\n",
    "  data = train_data(last_checkpoint_path)\n",
    "  df = pd.DataFrame(data)\n",
    "  df.to_csv(f\"{dir}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ced60c-7bd8-4369-a4d1-2721410eda52",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f955b-1249-4852-93b6-29f182244e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure_file in os.listdir(\"measures\"):\n",
    "  if \"csv\" not in measure_file:\n",
    "    continue\n",
    "  df: pd.DataFrame = pd.read_csv(os.path.join(\"measures\", measure_file))\n",
    "  data = {}\n",
    "  for column in df.columns:\n",
    "    if column == \"val_loss\":\n",
    "      data[\"val/loss\"] = df[\"val_loss\"]\n",
    "    else:\n",
    "      data[\"train/\" + column] = df[column]\n",
    "  pd.DataFrame(data).to_csv(measure_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef364fb1-afa1-450f-a16d-2b46042282cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs1 = [\"fasterrcnn-full\", \"fasterrcnn-head\", \"retinanet-full\", \"retinanet-head\", \"retinanet-split-b1e-04-h1e-02\"]\n",
    "logs2 = [log + \"-measure\" for log in logs1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d1169-6780-49a7-bacf-3d7b1df9e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvs1 = [log + \".csv\" for log in logs1]\n",
    "csvs2 = [log + \".csv\" for log in logs2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cb177e-079d-4a21-bb62-573cf2da3dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv1, csv2 in zip(csvs1, csvs2):\n",
    "  df1 = pd.read_csv(csv1)\n",
    "  df2 = pd.read_csv(csv2)\n",
    "\n",
    "  columns1 = df1.columns.tolist()\n",
    "  columns2 = df2.columns.tolist()\n",
    "\n",
    "  data = {}\n",
    "\n",
    "  for column in sorted(columns1 + columns2):\n",
    "    data[column] = df1[column] if column in columns1 else df2[column]\n",
    "\n",
    "  filename = csv1.split(\".\")[0] + \"-all.csv\"\n",
    "  pd.DataFrame(data).to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a34dfb6-7601-42ca-bca4-fb1db038925c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
